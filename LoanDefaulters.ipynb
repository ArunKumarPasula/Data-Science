{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SVR\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (0,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "loans_2007 = pd.read_csv(r'C:\\Users\\SVR\\Documents\\LOAN\\LoanStats3a.csv', skiprows=1)\n",
    "half_count = len(loans_2007) / 2\n",
    "loans_2007 = loans_2007.dropna(thresh=half_count, axis=1)\n",
    "loans_2007 = loans_2007.drop(['desc', 'url'],axis=1)\n",
    "loans_2007.to_csv(r'C:\\Users\\SVR\\Documents\\LOAN\\loans_2007.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SVR\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "loans_2007 = pd.read_csv(r'C:\\Users\\SVR\\Documents\\LOAN\\loans_2007.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us understand each and every feature and determine the final set of features that we can use for our problem.We even need to select our target feature from the pool of features which we feel would be the most appropriate one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are many features,let us analyze by splitting the features into sets of 18.Let us start with first set 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id: randomly generated field by Lending Club for unique identification purposes only\n",
    "member_id: also a randomly generated field by Lending Club for unique identification purposes only\n",
    "funded_amnt: leaks data from the future (after the loan is already started to be funded)\n",
    "funded_amnt_inv: also leaks data from the future (after the loan is already started to be funded)\n",
    "grade: contains redundant information as the interest rate column (int_rate)\n",
    "sub_grade: also contains redundant information as the interest rate column (int_rate)\n",
    "emp_title: requires other data and a lot of processing to potentially be useful\n",
    "issue_d: leaks data from the future (after the loan is already completed funded)\n",
    "Recall that Lending Club assigns a grade and a sub-grade based on the borrower's interest rate. While the grade and sub_grade values are categorical, the int_rate column contains continuous values, which are better suited for machine learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# droping colums of set 1 which are not required\n",
    "\n",
    "cols_to_drop = [\"id\", \"member_id\", \"funded_amnt\", \"funded_amnt_inv\", \"grade\", \"sub_grade\", \"emp_title\", \"issue_d\"]\n",
    "loans_2007.drop(cols_to_drop,axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second set of features:\n",
    "\n",
    "zip_code: redundant with the addr_state column since only the first 3 digits of the 5 digit zip code are visible (which only can be used to identify the state the borrower lives in)\n",
    "out_prncp: leaks data from the future, (after the loan already started to be paid off)\n",
    "out_prncp_inv: also leaks data from the future, (after the loan already started to be paid off)\n",
    "total_pymnt: also leaks data from the future, (after the loan already started to be paid off)\n",
    "total_pymnt_inv: also leaks data from the future, (after the loan already started to be paid off)\n",
    "total_rec_prncp: also leaks data from the future, (after the loan already started to be paid off)\n",
    "The out_prncp and out_prncp_inv both describe the outstanding principal amount for a loan, which is the remaining amount the borrower still owes. These 2 columns as well as the total_pymnt column describe properties of the loan after it's fully funded and started to be paid off. This information isn't available to an investor before the loan is fully funded and we don't want to include it in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# droping colums of set 2 which are not required\n",
    "cols_to_drop_set2 = [\"zip_code\", \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\", \"total_pymnt_inv\", \"total_rec_prncp\"]\n",
    "loans_2007.drop(cols_to_drop_set2,inplace = True,axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "third set of features:\n",
    "    \n",
    "total_rec_int: leaks data from the future, (after the loan already started to be paid off),\n",
    "total_rec_late_fee: also leaks data from the future, (after the loan already started to be paid off),\n",
    "recoveries: also leaks data from the future, (after the loan already started to be paid off),\n",
    "collection_recovery_fee: also leaks data from the future, (after the loan already started to be paid off),\n",
    "last_pymnt_d: also leaks data from the future, (after the loan already started to be paid off),\n",
    "last_pymnt_amnt: also leaks data from the future, (after the loan already started to be paid off).\n",
    "All of these columns leak data from the future, meaning that they're describing aspects of the loan after it's already been fully funded and started to be paid off by the borrower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#droping colums of set 2 which are not required\n",
    "\n",
    "cols_to_drop_set3 = [\"total_rec_int\", \"total_rec_late_fee\", \"recoveries\", \"collection_recovery_fee\", \"last_pymnt_d\", \"last_pymnt_amnt\"]\n",
    "loans_2007.drop(cols_to_drop_set3,axis =1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deciding on target column:\n",
    "We should use the loan_status column, since it's the only column that directly describes if a loan was paid off on time, had delayed payments, or was defaulted on the borrower. \n",
    "Currently, this column contains text values and we need to convert it to a numerical one for training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully Paid                                             33902\n",
      "Charged Off                                             5658\n",
      "Does not meet the credit policy. Status:Fully Paid      1988\n",
      "Does not meet the credit policy. Status:Charged Off      761\n",
      "Current                                                  201\n",
      "Late (31-120 days)                                        10\n",
      "In Grace Period                                            9\n",
      "Late (16-30 days)                                          5\n",
      "Default                                                    1\n",
      "Name: loan_status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(loans_2007['loan_status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above tells that there are many categories loan_status column.\n",
    "From the investor's perspective, we're interested in trying to predict \n",
    "which loans will be paid off on time and which ones won't be. \n",
    "Only the Fully Paid and Charged Off values describe the final outcome of the loan. \n",
    "The other values describe loans that are still on going and where the jury is still out \n",
    "on if the borrower will pay back the loan on time or not\n",
    "\n",
    "Fully Paid - Loan has been fully paid off.\n",
    "Charged Off\t- Loan for which there is no longer a reasonable expectation of further payments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're interesting in being able to predict which of these 2 values a loan will fall under,\n",
    "we can treat the problem as a binary classification one.\n",
    "Let's remove all the loans that don't contain either Fully Paid and Charged Off as the loan's status and then\n",
    "transform the Fully Paid values to 1 for the positive case\n",
    "and the Charged Off values to 0 for the negative case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove all rows from loans_2007 that contain values other than Fully Paid or Charged Off for the loan_status column\n",
    "loans_2007 = loans_2007[(loans_2007['loan_status'] == 'Fully Paid') | (loans_2007['loan_status'] == 'Charged Off')]\n",
    "# above filters rows which matches our condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As target column valyes are categorical,will convert into numerical,i.e., converting our problem as binary classification with Fully Paid = 1 and Charged off = \n",
    "loan_status = {'loan_status':{'Fully Paid' : 1, 'Charged Off' : 0}}\n",
    "loans_2007.replace(loan_status,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['initial_list_status', 'collections_12_mths_ex_med', 'policy_code', 'application_type', 'acc_now_delinq', 'chargeoff_within_12_mths', 'delinq_amnt', 'tax_liens']\n"
     ]
    }
   ],
   "source": [
    "# lets look at the columns that has only one value,i.e., same value for every loan application.This type of columns are of no use,so,lets remove such columns\n",
    "drop_columns  = []\n",
    "for col in loans_2007.columns:\n",
    "    non_null = loans_2007[col].dropna() # beacuse series unique() method considers pandas nan as 0,so applying this method has chance of returning more than 1 unique value,to avoid that will remove the entries that has NaNs   \n",
    "    unique_count = len(non_null.unique())\n",
    "    #print(unique_count)\n",
    "    if unique_count == 1:\n",
    "        drop_columns.append(col)\n",
    "#print(drop_columns)\n",
    "loans_2007.drop(drop_columns,axis =1,inplace = True)\n",
    "print(drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the things done till now:\n",
    "we started to become familiar with the columns in the dataset and removed many columns that aren't useful for modeling by taking various things into consideration (columns that have redundant information,colums that has data which requires too much processing to make useful,columns that has information of the future (we don't consider as this kind of information results in undesired model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us prepare the data for machine learning by focusing on handling missing values, converting categorical columns to numeric columns, and removing any other extraneous columns we encounter throughout this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_amnt                 0\n",
      "term                      0\n",
      "int_rate                  0\n",
      "installment               0\n",
      "emp_length                0\n",
      "home_ownership            0\n",
      "annual_inc                0\n",
      "verification_status       0\n",
      "loan_status               0\n",
      "pymnt_plan                0\n",
      "purpose                   0\n",
      "title                    10\n",
      "addr_state                0\n",
      "dti                       0\n",
      "delinq_2yrs               0\n",
      "earliest_cr_line          0\n",
      "inq_last_6mths            0\n",
      "open_acc                  0\n",
      "pub_rec                   0\n",
      "revol_bal                 0\n",
      "revol_util               50\n",
      "total_acc                 0\n",
      "last_credit_pull_d        2\n",
      "pub_rec_bankruptcies    697\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "null_counts = loans_2007.isnull().sum() #isnull() returns True if value is missing and sum() considers  True as 1,False as 0\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove pub_rec_bankruptcies columns as it has many missing values\n",
    "#removing rows where the columns title,revol_util and last_credit_pull_d has missing values\n",
    "#loans_2007.drop('pub_rec_bankruptcies',axis =1 , inplace = True)\n",
    "loans_2007.dropna(axis =0,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object     12\n",
      "float64    11\n",
      "int64       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#let us check the data types of the columns,text columns need to be converted into numerical values as machine learning models accept only numerical values\n",
    "print(loans_2007.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         term int_rate emp_length home_ownership verification_status  \\\n",
      "0   36 months   10.65%  10+ years           RENT            Verified   \n",
      "\n",
      "  pymnt_plan      purpose     title addr_state earliest_cr_line revol_util  \\\n",
      "0          n  credit_card  Computer         AZ         Jan-1985      83.7%   \n",
      "\n",
      "  last_credit_pull_d  \n",
      "0           Nov-2016  \n"
     ]
    }
   ],
   "source": [
    "#selecting object columns\n",
    "object_columns_df = loans_2007.select_dtypes(include=['object'])\n",
    "print(object_columns_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us have a look on the text columns\n",
    "\n",
    "home_ownership: home ownership status, can only be 1 of 4 categorical values according to the data dictionary,\n",
    "verification_status: indicates if income was verified by Lending Club,\n",
    "emp_length: number of years the borrower was employed upon time of application,\n",
    "term: number of payments on the loan, either 36 or 60,\n",
    "addr_state: borrower's state of residence,\n",
    "purpose: a category provided by the borrower for the loan request,\n",
    "title: loan title provided the borrower,\n",
    "int_rate: interest rate of the loan in %,\n",
    "revol_util: revolving line utilization rate or the amount of credit the borrower is using relative to all available credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19145\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(loans_2007['title'].value_counts()))\n",
    "print(len(loans_2007['purpose'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RENT        18400\n",
      "MORTGAGE    17323\n",
      "OWN          2983\n",
      "OTHER          96\n",
      "Name: home_ownership, dtype: int64\n",
      "Not Verified       16161\n",
      "Verified           12704\n",
      "Source Verified     9937\n",
      "Name: verification_status, dtype: int64\n",
      "10+ years    8702\n",
      "< 1 year     4397\n",
      "2 years      4287\n",
      "3 years      4013\n",
      "4 years      3364\n",
      "5 years      3237\n",
      "1 year       3135\n",
      "6 years      2180\n",
      "7 years      1734\n",
      "8 years      1447\n",
      "9 years      1237\n",
      "n/a          1069\n",
      "Name: emp_length, dtype: int64\n",
      " 36 months    28345\n",
      " 60 months    10457\n",
      "Name: term, dtype: int64\n",
      "CA    6988\n",
      "NY    3701\n",
      "FL    2804\n",
      "TX    2668\n",
      "NJ    1805\n",
      "IL    1505\n",
      "PA    1495\n",
      "GA    1368\n",
      "VA    1367\n",
      "MA    1298\n",
      "OH    1191\n",
      "MD    1024\n",
      "AZ     862\n",
      "WA     811\n",
      "CO     763\n",
      "NC     749\n",
      "CT     737\n",
      "MI     713\n",
      "MO     674\n",
      "MN     604\n",
      "NV     489\n",
      "SC     462\n",
      "OR     439\n",
      "AL     439\n",
      "WI     436\n",
      "LA     426\n",
      "KY     323\n",
      "OK     297\n",
      "KS     267\n",
      "UT     250\n",
      "AR     240\n",
      "DC     208\n",
      "RI     196\n",
      "NM     182\n",
      "WV     176\n",
      "HI     171\n",
      "NH     169\n",
      "DE     111\n",
      "MT      84\n",
      "WY      82\n",
      "AK      78\n",
      "SD      62\n",
      "VT      52\n",
      "MS      19\n",
      "TN      10\n",
      "ID       4\n",
      "NE       1\n",
      "IN       1\n",
      "IA       1\n",
      "Name: addr_state, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#From above,as both more or less same information(just differnt wordings),and as purpose columns contains few discrete values,will conside purpose columns and remove title column\n",
    "# also let us look for unnique values of other non numeric columns\n",
    "cols = ['home_ownership', 'verification_status', 'emp_length', 'term', 'addr_state']\n",
    "for col in cols:\n",
    "    print(loans_2007[col].value_counts())\n",
    "#The home_ownership, verification_status, emp_length, and term columns each contain a few discrete categorical values. #\n",
    "#We should encode these columns as dummy variables and keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SVR\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3312: UserWarning: the \"axis\" argument is deprecated and will be removed inv0.13; this argument has no effect\n",
      "  warn('the \"axis\" argument is deprecated and will be removed in'\n"
     ]
    }
   ],
   "source": [
    "#now let us work at emp_length column\n",
    "loans_2007['emp_length']\n",
    "#We assume that people who may have been working more than 10 years have only really worked for 10 years. We also assume that people who've worked less than a year or if the information is not available that they've worked for 0. This is a general heuristic but it's not perfect.\n",
    "mapping_dict = {\n",
    "        \"emp_length\": {\n",
    "            \"10+ years\": 10,\n",
    "            \"9 years\": 9,\n",
    "            \"8 years\": 8,\n",
    "            \"7 years\": 7,\n",
    "            \"6 years\": 6,\n",
    "            \"5 years\": 5,\n",
    "            \"4 years\": 4,\n",
    "            \"3 years\": 3,\n",
    "            \"2 years\": 2,\n",
    "            \"1 year\": 1,\n",
    "            \"< 1 year\": 0,\n",
    "            \"n/a\": 0\n",
    "        }\n",
    "    }\n",
    "loans_2007.replace(mapping_dict,axis = 1,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "103\n",
      "526\n"
     ]
    }
   ],
   "source": [
    "# let us look at addr_state column\n",
    "print(len(loans_2007['addr_state'].value_counts()))\n",
    "print(len(loans_2007['last_credit_pull_d'].value_counts()))\n",
    "print(len(loans_2007['earliest_cr_line'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#As there are more categories for the columns addr_state,last_credit_pull_d and earliest_cr_line,let us remove them as encoding the into numerical values is not viable\n",
    "cols = ['last_credit_pull_d', 'addr_state', 'title','earliest_cr_line','pymnt_plan']\n",
    "loans_2007.drop(cols,axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now its turn to have a look at int_rate column\n",
    "#loans_2007['revol_util']\n",
    "# as the values have '%' at the end ,we need to strip that symbol and convert into float type\n",
    "for i in ['int_rate','revol_util']:\n",
    "    loans_2007[i]=loans_2007[i].str.rstrip('%')\n",
    "    loans_2007[i] = loans_2007[i].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's now encode the home_ownership, verification_status, title, and term columns as dummy variables so we can use them in our model. We first need to use the Pandas get_dummies method to return a new Dataframe containing a new column for each dummy variable:\n",
    "# Returns a new Dataframe containing 1 column for each dummy variable.\n",
    "\n",
    " \n",
    "cat_columns = [\"home_ownership\", \"verification_status\", \"emp_length\", \"purpose\", \"term\"]\n",
    "dummy_df = pd.get_dummies(loans_2007[cat_columns])\n",
    "loans = pd.concat([loans_2007, dummy_df], axis=1)\n",
    "loans = loans.drop(cat_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "#now we are almost done with the cleaning up of the data.Now let us see how many predictor variables are there in the final data on which we apply our machine learning model\n",
    "\n",
    "print(len(loans.columns))\n",
    "#so,out of 38 columns,7 are predictor variables and one is target variable.The algorithm will make predictions about whether or not a loan will be paid off on time, which is contained in the loan_status column of the dataset.\n",
    "#if we have a look at the data,clearly the data is imbalanced,as There are about 6 times as many loans that were paid off on time (positive case, label of 1) than those that weren't (negative case, label of 0)\n",
    "#Imbalances can cause issues with many machine learning algorithms, where they appear to have high accuracy, but actually aren't learning from the training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERROR METRIC\n",
    "Before diving in and selecting an algorithm to apply to the data, we should select an error metric.\n",
    "An error metric will help us figure out when our model is performing well, and when it's performing poorly.\n",
    "let's say we're using a machine learning model to predict whether or not we should fund a loan on the Lending Club platform. Our objective in this is to make money -- we want to fund enough loans that are paid off on time to offset our losses from loans that aren't paid off. An error metric will help us determine if our algorithm will make us money or lose us money.\n",
    "\n",
    "FASLE POSITIVES AND FALSE NEGATIVES\n",
    "In this case, we're primarily concerned with false positives and false negatives. \n",
    "Both of these are different types of misclassifications.\n",
    "With a false positive, we predict that a loan will be paid off on time, but it actually isn't. This costs us money, since we fund loans that lose us money. With a false negative, we predict that a loan won't be paid off on time, but it actually would be paid off on time. This loses us potential money, since we didn't fund a loan that actually would have been paid off.\n",
    "\n",
    "WHICH ONE TO TAKE CARE OF?\n",
    "Since we're viewing this problem from the standpoint of a conservative investor, we need to treat false positives differently than false negatives. A conservative investor would want to minimize risk, and avoid false positives as much as possible. They'd be more okay with missing out on opportunities (false negatives) than they would be with funding a risky loan (false positives).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to optimize for\n",
    "1.high recall (true positive rate(TPR))\n",
    "2.low fall-out (false positive rate(FPR))\n",
    "\n",
    "1.TPR\n",
    "True positive rate is the number of true positives divided by the number of true positives plus the number of false negatives. This divides all the cases where we thought a loan would be paid off and it was by all the loans that were paid off:\n",
    "tpr = tp / (tp + fn)\n",
    "\n",
    "2.FPR\n",
    "False positive rate is the number of false positives divided by the number of false positives plus the number of true negatives. This divides all the cases where we thought a loan would be paid off but it wasn't by all the loans that weren't paid off:\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "\n",
    "False Positive Rate -- \"what percentage of my 1 predictions are incorrect?\"\n",
    "In this case, \"what percentage of the loans that I fund would not be repaid?\"\n",
    "\n",
    "True Positive Rate -- \"what percentage of all the possible 1 predictions am I making?\"\n",
    "In this case, \"what percentage of loans that could be funded would I fund?\"\n",
    "\n",
    "Generally, if we want to reduce false positive rate, true positive rate will also go down. This is because if we want to reduce the risk of false positives, we wouldn't think about funding riskier loans in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TIME TO USE MACHINE LEARNING MODEL TO MODEL OUR DATA\n",
    "#A good first algorithm to apply to binary classification problems is logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "cols = loans.columns\n",
    "train_cols = cols.drop(\"loan_status\")\n",
    "features = loans[train_cols]\n",
    "target = loans[\"loan_status\"]\n",
    "lr.fit(features, target)\n",
    "predictions = lr.predict(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CROSS VALIDATION:\n",
    "\n",
    "-While we generated predictions in the last screen, those predictions were overfit. They were overfit because we generated predictions using the same data that we trained our model on. When we use this to evaluate error, we get an unrealistically high depiction of how accurate the algorithm is, because it already \"knows\" the correct answers.\n",
    "\n",
    "-In order to get a realistic depiction of the accuracy of the algorithm, we'll need to use cross validation to generate predictions. Cross validation splits the dataset into groups, then makes predictions on each group using the other groups as training data. This ensures that we don't overfit by generating predictions on the same data that we train our algorithm with.\n",
    "\n",
    "-We can perform cross validation using the cross_val_predict method of scikit-learn. cross_val_predict allows us to pass in a classifier, the features, and the target.\n",
    "\n",
    "-We'll create an instance of KFold, which will perform 3 fold cross validation across our dataset. We set random_state to 1 to ensure that the folds are always consistent, and we can compare scores between runs. If we don't, each fold will be randomized every time, making it hard to tell if we're improving our model or not.\n",
    "\n",
    "-If we pass the instance of KFold into cross_val_predict, it will then perform 3 fold cross validation to generate unbiased predictions.\n",
    "\n",
    "-Once we have cross validated predictions, we can compute true positive rate and false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_predict, KFold\n",
    "kf = KFold(features.shape[0], random_state=1)\n",
    "predictions = cross_val_predict(lr, features, target, cv=kf)\n",
    "predictions = pd.Series(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9991828087167071\n",
      "0.9990864242645715\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# False positives.\n",
    "fp_filter = (predictions == 1) & (loans[\"loan_status\"] == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "# True positives.\n",
    "tp_filter = (predictions == 1) & (loans[\"loan_status\"] == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "# False negatives.\n",
    "fn_filter = (predictions == 0) & (loans[\"loan_status\"] == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "# True negatives\n",
    "tn_filter = (predictions == 0) & (loans[\"loan_status\"] == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "# Rates\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(tpr)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted value counts : \n",
      " 1    38770\n",
      "0       32\n",
      "dtype: int64 \n",
      "----------------------------------------------------------------\n",
      "actual value counts : \n",
      " 1    33281\n",
      "0     5521\n",
      "Name: loan_status, dtype: int64 \n"
     ]
    }
   ],
   "source": [
    "print('predicted value counts : \\n %s '%(predictions.value_counts()))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print('actual value counts : \\n %s '%(loans['loan_status'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above,we can see that there are many 0's classified as 1's.This is because,the dataset is imbalanced as there are more number of samples classified as class 1 than class 0.\n",
    "\n",
    "Two methods to overcome this proble\n",
    "1.oversampling and undersampling\n",
    "2.Tell the classifier to penalize misclassifications of the less prevalent class more than the other class\n",
    "\n",
    "1.oversampling and undersampling\n",
    "oversampling-Copy rows multiple times. One way to equalize the 0s and 1s is to copy rows where loan_status is 0\n",
    "undersampling- we could do this by deleting rows where loan_status is 1.\n",
    "\n",
    "-Unfortunately, none of these techniques are especially easy. The second method we mentioned earlier, telling the classifier to penalize certain rows more, is actually much easier to implement using scikit-learn\n",
    "\n",
    "-We can do this by setting the class_weight parameter to balanced when creating the LogisticRegression instance. \n",
    "\n",
    "-This tells scikit-learn to penalize the misclassification of the minority class during the training process. \n",
    "\n",
    "-The penalty means that the logistic regression classifier pays more attention to correctly classifying rows where loan_status is 0. \n",
    "\n",
    "-This lowers accuracy when loan_status is 1, but raises accuracy when loan_status is 0.\n",
    "\n",
    "-By setting the class_weight parameter to balanced, the penalty is set to be inversely proportional to the class frequencies.\n",
    "\n",
    "-This would mean that for the classifier, correctly classifying a row where loan_status is 0 is 6 times more important than correctly classifying a row where loan_status is 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MANUAL PENALTIES\n",
    "We significantly improved false positive rate in the last screen by balancing the classes, which reduced true positive rate. \n",
    "Our true positive rate is now around 67%, and our false positive rate is around 40%. From a conservative investor's standpoint, it's reassuring that the false positive rate is lower because it means that we'll be able to do a better job at avoiding bad loans than if we funded everything.\n",
    "However, we'd only ever decide to fund 67% of the total loans (true positive rate), so we'd immediately reject a good amount of loans\n",
    "\n",
    "We can try to lower the false positive rate further by assigning a harsher penalty for misclassifying the negative class. \n",
    "While setting class_weight to balanced will automatically set a penalty based on the number of 1s and 0s in the column, we can also set a manual penalty.\n",
    "In the last screen, the penalty scikit-learn imposed for misclassifying a 0 would have been around 5.89 (since there are 5.89 times as many 1s as 0s).\n",
    "\n",
    "We can also specify a penalty manually if we want to adjust the rates more. To do this, we need to pass in a dictionary of penalty values to the class_weight parameter:\n",
    "\n",
    "penalty = {\n",
    "    0: 10,\n",
    "    1: 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6267554479418886\n",
      "0.6144710396491869\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "kf = KFold(features.shape[0], random_state=1)\n",
    "predictions = cross_val_predict(lr, features, target, cv=kf)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "# False positives.\n",
    "fp_filter = (predictions == 1) & (loans[\"loan_status\"] == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "# True positives.\n",
    "tp_filter = (predictions == 1) & (loans[\"loan_status\"] == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "# False negatives.\n",
    "fn_filter = (predictions == 0) & (loans[\"loan_status\"] == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "# True negatives\n",
    "tn_filter = (predictions == 0) & (loans[\"loan_status\"] == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "# Rates\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(tpr)\n",
    "print(fpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.225635593220339\n",
      "0.22071989767951763\n"
     ]
    }
   ],
   "source": [
    "penalty = {\n",
    "    0: 10,\n",
    "    1: 1\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(class_weight=penalty)\n",
    "kf = KFold(features.shape[0], random_state=1)\n",
    "predictions = cross_val_predict(lr, features, target, cv=kf)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "# False positives.\n",
    "fp_filter = (predictions == 1) & (loans[\"loan_status\"] == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "# True positives.\n",
    "tp_filter = (predictions == 1) & (loans[\"loan_status\"] == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "# False negatives.\n",
    "fn_filter = (predictions == 0) & (loans[\"loan_status\"] == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "# True negatives\n",
    "tn_filter = (predictions == 0) & (loans[\"loan_status\"] == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "# Rates\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(tpr)\n",
    "print(fpr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observations from the above output:\n",
    "1.when we set penalty to balanced the tpr and fpr values are 0.6267554479418886\n",
    "0.6144710396491869 respectively\n",
    "\n",
    "2.Next we tried to impose more penalty on misclassification of  class 0 using values defined by dictionary penalty.\n",
    "\n",
    "3.Both tpr and fpr values are around 22%.It means,we have reduced fpr but at the same time tpr values are also decreased which is not desired\n",
    "\n",
    "4.While we have fewer false positives, we're also missing opportunities to fund more loans and potentially make more money. Given that we're approaching this as a conservative investor, this strategy makes sense, but it's worth keeping in mind the tradeoffs.\n",
    "\n",
    "5.While we could tweak the penalties further, it's best to move to trying a different model right now, for larger potential false positive rate gains\n",
    "\n",
    "6.Let's try a more complex algorithm, random forest. Random forests are able to work with nonlinear data, and learn complex conditionals. Logistic regressions are only able to work with linear data. Training a random forest algorithm may enable us to get more accuracy due to columns that correlate nonlinearly with loan_status.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "rf = RandomForestClassifier(class_weight=\"balanced\", random_state=1)\n",
    "kf = KFold(features.shape[0], random_state=1)\n",
    "predictions = cross_val_predict(rf, features, target, cv=kf)\n",
    "predictions = pd.Series(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9707627118644068\n",
      "0.9700347158779463\n"
     ]
    }
   ],
   "source": [
    "# False positives.\n",
    "fp_filter = (predictions == 1) & (loans[\"loan_status\"] == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "# True positives.\n",
    "tp_filter = (predictions == 1) & (loans[\"loan_status\"] == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "# False negatives.\n",
    "fn_filter = (predictions == 0) & (loans[\"loan_status\"] == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "# True negatives\n",
    "tn_filter = (predictions == 0) & (loans[\"loan_status\"] == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "# Rates\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(tpr)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, using a random forest classifier didn't improve our false positive rate. The model is likely weighting too heavily on the 1 class, and still mostly predicting 1s. We could fix this by applying a harsher penalty for misclassifications of 0s.\n",
    "Ultimately, our best model had a false positive rate of 7%, and a true positive rate of 20%. For a conservative investor, this means that they make money as long as the interest rate is high enough to offset the losses from 7% of borrowers defaulting, and that the pool of 20% of borrowers is large enough to make enough interest money to offset the losses.\n",
    "\n",
    "If we had randomly picked loans to fund, borrowers would have defaulted on 14.5% of them, and our model is better than that, although we're excluding more loans than a random strategy would. Given this, there's still quite a bit of room to improve:\n",
    "\n",
    "We can tweak the penalties further.\n",
    "We can try models other than a random forest and logistic regression.\n",
    "We can use some of the columns we discarded to generate better features.\n",
    "We can ensemble multiple models to get more accurate predictions.\n",
    "We can tune the parameters of the algorithm to achieve higher performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
